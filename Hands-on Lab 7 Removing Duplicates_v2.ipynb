{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "Empty DataFrame\n",
      "Columns: [ResponseId, MainBranch, Age, Employment, RemoteWork, Check, CodingActivities, EdLevel, LearnCode, LearnCodeOnline, TechDoc, YearsCode, YearsCodePro, DevType, OrgSize, PurchaseInfluence, BuyNewTool, BuildvsBuy, TechEndorse, Country, Currency, CompTotal, LanguageHaveWorkedWith, LanguageWantToWorkWith, LanguageAdmired, DatabaseHaveWorkedWith, DatabaseWantToWorkWith, DatabaseAdmired, PlatformHaveWorkedWith, PlatformWantToWorkWith, PlatformAdmired, WebframeHaveWorkedWith, WebframeWantToWorkWith, WebframeAdmired, EmbeddedHaveWorkedWith, EmbeddedWantToWorkWith, EmbeddedAdmired, MiscTechHaveWorkedWith, MiscTechWantToWorkWith, MiscTechAdmired, ToolsTechHaveWorkedWith, ToolsTechWantToWorkWith, ToolsTechAdmired, NEWCollabToolsHaveWorkedWith, NEWCollabToolsWantToWorkWith, NEWCollabToolsAdmired, OpSysPersonal use, OpSysProfessional use, OfficeStackAsyncHaveWorkedWith, OfficeStackAsyncWantToWorkWith, OfficeStackAsyncAdmired, OfficeStackSyncHaveWorkedWith, OfficeStackSyncWantToWorkWith, OfficeStackSyncAdmired, AISearchDevHaveWorkedWith, AISearchDevWantToWorkWith, AISearchDevAdmired, NEWSOSites, SOVisitFreq, SOAccount, SOPartFreq, SOHow, SOComm, AISelect, AISent, AIBen, AIAcc, AIComplex, AIToolCurrently Using, AIToolInterested in Using, AIToolNot interested in Using, AINextMuch more integrated, AINextNo change, AINextMore integrated, AINextLess integrated, AINextMuch less integrated, AIThreat, AIEthics, AIChallenges, TBranch, ICorPM, WorkExp, Knowledge_1, Knowledge_2, Knowledge_3, Knowledge_4, Knowledge_5, Knowledge_6, Knowledge_7, Knowledge_8, Knowledge_9, Frequency_1, Frequency_2, Frequency_3, TimeSearching, TimeAnswering, Frustration, ProfessionalTech, ProfessionalCloud, ProfessionalQuestion, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "Duplicates = df[df.duplicated()]\n",
    "print(\"Number of duplicate rows:\", Duplicates.shape[0])\n",
    "print(Duplicates.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows before removal: 0\n",
      "Duplicate rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "initial_duplicates = df.duplicated().sum()\n",
    "print(\"Duplicate rows before removal:\", initial_duplicates)\n",
    "\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "final_duplicates = df_cleaned.duplicated().sum()\n",
    "print(\"Duplicate rows after removal:\", final_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "RemoteWork             10631\n",
      "CodingActivities       10971\n",
      "EdLevel                 4653\n",
      "LearnCode               4949\n",
      "LearnCodeOnline        16200\n",
      "                       ...  \n",
      "JobSatPoints_11        35992\n",
      "SurveyLength            9255\n",
      "SurveyEase              9199\n",
      "ConvertedCompYearly    42002\n",
      "JobSat                 36311\n",
      "Length: 109, dtype: int64\n",
      "Missing values before imputation of EdLevel: 4653\n",
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values_filtered = missing_values[missing_values > 0]\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values_filtered)\n",
    "\n",
    "print(\"Missing values before imputation of EdLevel:\", df['EdLevel'].isnull().sum())\n",
    "most_frequent = df['EdLevel'].mode()[0]\n",
    "df['EdLevel'].fillna(most_frequent, inplace=True)\n",
    "print(\"Missing values after imputation:\", df['EdLevel'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics before handling missing values:\n",
      "count    2.343500e+04\n",
      "mean     8.615529e+04\n",
      "std      1.867570e+05\n",
      "min      1.000000e+00\n",
      "25%      3.271200e+04\n",
      "50%      6.500000e+04\n",
      "75%      1.079715e+05\n",
      "max      1.625660e+07\n",
      "Name: ConvertedCompYearly, dtype: float64\n",
      "\n",
      "Normalized compensation column preview:\n",
      "     ConvertedCompYearly  NormalizedComp\n",
      "72                7322.0        0.000450\n",
      "374              30074.0        0.001850\n",
      "379              91295.0        0.005616\n",
      "385              53703.0        0.003303\n",
      "389             110000.0        0.006766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1061/847551785.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['NormalizedComp'] = (df_cleaned['ConvertedCompYearly'] - comp_min) / (comp_max - comp_min)\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary statistics before handling missing values:\")\n",
    "print(df['ConvertedCompYearly'].describe())\n",
    "df_cleaned = df[df['ConvertedCompYearly'].notnull()]\n",
    "\n",
    "comp_min = df_cleaned['ConvertedCompYearly'].min()\n",
    "comp_max = df_cleaned['ConvertedCompYearly'].max()\n",
    "\n",
    "df_cleaned['NormalizedComp'] = (df_cleaned['ConvertedCompYearly'] - comp_min) / (comp_max - comp_min)\n",
    "\n",
    "print(\"\\nNormalized compensation column preview:\")\n",
    "print(df_cleaned[['ConvertedCompYearly', 'NormalizedComp']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ResponseId                                         MainBranch  \\\n",
      "72           73                     I am a developer by profession   \n",
      "374         375  I am not primarily a developer, but I write co...   \n",
      "379         380                     I am a developer by profession   \n",
      "385         386                     I am a developer by profession   \n",
      "389         390                     I am a developer by profession   \n",
      "\n",
      "                 Age                                         Employment  \\\n",
      "72   18-24 years old  Employed, full-time;Student, full-time;Indepen...   \n",
      "374  25-34 years old                                Employed, full-time   \n",
      "379  35-44 years old                                Employed, full-time   \n",
      "385  35-44 years old  Independent contractor, freelancer, or self-em...   \n",
      "389  25-34 years old             Employed, full-time;Student, part-time   \n",
      "\n",
      "                               RemoteWork   Check  \\\n",
      "72   Hybrid (some remote, some in-person)  Apples   \n",
      "374  Hybrid (some remote, some in-person)  Apples   \n",
      "379                                Remote  Apples   \n",
      "385                                Remote  Apples   \n",
      "389                                Remote  Apples   \n",
      "\n",
      "                                      CodingActivities  \\\n",
      "72   Hobby;School or academic work;Professional dev...   \n",
      "374  Hobby;School or academic work;Professional dev...   \n",
      "379                     Hobby;Bootstrapping a business   \n",
      "385                                              Hobby   \n",
      "389                      Hobby;School or academic work   \n",
      "\n",
      "                                               EdLevel  \\\n",
      "72   Secondary school (e.g. American high school, G...   \n",
      "374     Professional degree (JD, MD, Ph.D, Ed.D, etc.)   \n",
      "379    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "385    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "389  Some college/university study without earning ...   \n",
      "\n",
      "                                             LearnCode  \\\n",
      "72   On the job training;Other online resources (e....   \n",
      "374  Books / Physical media;Colleague;On the job tr...   \n",
      "379  Books / Physical media;Other online resources ...   \n",
      "385  Books / Physical media;On the job training;Oth...   \n",
      "389  Books / Physical media;Colleague;On the job tr...   \n",
      "\n",
      "                                       LearnCodeOnline  ... JobSatPoints_7  \\\n",
      "72   Technical documentation;Blogs;Written Tutorial...  ...          100.0   \n",
      "374  Written Tutorials;Stack Overflow;Written-based...  ...            NaN   \n",
      "379  Technical documentation;Books;Social Media;Wri...  ...            0.0   \n",
      "385  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "389  Written Tutorials;Stack Overflow;Coding sessio...  ...           30.0   \n",
      "\n",
      "    JobSatPoints_8 JobSatPoints_9 JobSatPoints_10 JobSatPoints_11  \\\n",
      "72           100.0          100.0            50.0            90.0   \n",
      "374            NaN            NaN             NaN             NaN   \n",
      "379            0.0            0.0             0.0             0.0   \n",
      "385            NaN            NaN             NaN             NaN   \n",
      "389            5.0           20.0            10.0             5.0   \n",
      "\n",
      "              SurveyLength                  SurveyEase ConvertedCompYearly  \\\n",
      "72                Too long                        Easy              7322.0   \n",
      "374  Appropriate in length  Neither easy nor difficult             30074.0   \n",
      "379               Too long                   Difficult             91295.0   \n",
      "385              Too short                        Easy             53703.0   \n",
      "389               Too long                        Easy            110000.0   \n",
      "\n",
      "    JobSat NormalizedComp  \n",
      "72    10.0       0.000450  \n",
      "374    NaN       0.001850  \n",
      "379   10.0       0.005616  \n",
      "385    NaN       0.003303  \n",
      "389   10.0       0.006766  \n",
      "\n",
      "[5 rows x 115 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
